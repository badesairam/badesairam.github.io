# Learning from Human Feedback 
Making language models bigger does not inherently make them better at following a user's intent. Wouldn't it be great if we use human feedback for generated text as a measure of performance or go even one step further and use that feedback to optimise the model for user's intent? That's the idead of Reinforcement Learning from Human Feedback (**RLHF**). RL has been very difficult to wotk with, and therefore, confined to gaming and simulated environments. 

In this post lets go over how exactly does RLHF work? why does it work? and intuition behind RLHF. Lets also use trlX library to implement RLHF for summarization task


## Language Model
A language model encodes statistical information about language. Statistical information tells us how likely a token(usually a sub-word) appear in a given context. It can be defined as

$P(nexttoken \mid prompt)$ 

Pretraining optimizes for completion. making LMs bigger does not inherently make them better at following a user's intent. For example, if you give pretrained model a question, ```How to make pizza```, any of the following could be a valid completion
1. Adding more context to the question: ```for a family of six```
2. Adding follow-up questions: ```?How much time would it take?
3. Actually giving the answer

We can use **supervised finetuning(SFT)** to optimize/align the pretrained model to generate the responses that users are looking for. We can use human-curated demonstration data for this, often referred as behavior cloning.

Demonstration data is hard to collect and as dialogues are often felxible resulting in plausible responses rather than a single answer for a prompt. So we would like the model to learn human-preference. Inorder to achieve this , we will employ Reinforce Learning

                                             
SFT and RLHF from InstructGPT paper
![InstructGPT](/assets/images/rlhf_images/instructGPT.png)


## Reinforcement Learning

Aim of Reinforcement Learning is how an intelligent agent should take actions in an environment to maximize the overall reward. In game systems reward achieved is very clear.

![RL system](/assets/images/rlhf_images/RL.png)


A policy rules how the agent selects the action to perform given the state $a_{t}$ ~ $\pi(.|s_{t})$ . 
Aim is to identify an optimal policy which maximizes the expected reward J given policy $\pi_\theta$, parametrized by parameters $\theta$
$$ \pi^* = argmax_\pi J(\pi_\theta) $$  
$$J(\pi) = E[R(\tau)] = \int_\tau P(\tau|\pi)R(\tau)$$

We can use stochastic gradient ascent to achieve this maxima 

**Policy Gradient optimisation**

$$ \theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\pi_\theta)| \theta_k$$

Probability of taking a trajectory within a policy $\pi$ can be defined as
$$ P(\tau|\pi_\theta) = P_0(S_0) \prod_{t=0}^T P(S_{t+1}|a_t, S_t) \pi_\theta(a_t|S_t) $$

We can simplicify policy gradient $\nabla_\theta J(\pi_\theta)$ using (attaching the derivation below)
$$\nabla_\theta J(\pi_\theta) = E[\sum_{t=0}^T \nabla_\theta log \pi_\theta(a_t|S_t) R(\tau)] $$

This expectation can be estimated as sample mean by collecting some $D= \{\tau_i\}$ trajectories
$$\hat{g} = 1/|D| \sum_{\tau \in D} \sum_{t=0}^T \nabla_\theta log \pi_\theta(a_t|S_t) R(\tau) $$

REINFORCE ALGORITHM for policy optimisation
1. Create a neural network with parameters $\theta$ that defines a policy
2. Use the neural network to sample trajectories and their rewards
3. Calculate the gradient $\hat{g}$ 
4. Run Stochastic Ascent to update parameters $\theta$
5. Repeat Step 2

Policy Gradient Simplification

![policy_gradient](/assets/images/rlhf_images/policy_gradient.png)

Now lets get back to our Langauge Model

## Language Model as an Agent
We can consider a language model as an agent with the current prompt representing state and the next token selection can be considered as an Action. The language model should be rewareded for generating "good responses (human preferred)".  

Policy is the langauge model itself

$a_{t}$ ~ $\pi(.|s_{t})$ = $P(nexttoken|prompt)$

States and actions in a Language model , $\tau = \{s_i\}$ 
![LM States](/assets/images/rlhf_images/LM_states.png)

But how to determine the reward(a scalar) that needs to given for a reponse for prompt ? We exploit the fact that us humans are good at comparing but not agreeing to reach a common ground. The idea is what if we have a scoring function that , if given a prompt and a reponse -> outputs a score for how "good" that reposnse. This scoring function called reward model or preference model calibrated with human preferences can be used to train our LLMs towards giving reponses with high scores. The output being a scalar reward is crucial for existing RL algorithms being integrated seamlessly later in the RLHF process.

RLHF process involves
1) Training a Reward model to generate a score given prompt, response pair
2) Optimize the LLM to generate responses for which the reward model will give high scores with a policy-gradient RL algorithm, Proximal Policy Optimization (PPO).

### Reward Model
Reward model can be both another fine-tuned LM or a LM trained from scratch on the preference data. For example, Anthropic has used a specialized method of fine-tuning to initialize these models after pretraining (preference model pretraining, PMP) because they found it to be more sample efficient than fine-tuning, but no one base model is considered the clear best choice for reward models. There will be one linear layer to produce 1 score

![RM Model](/assets/images/rlhf_images/RewardModel.png)

Human annotators are used to rank the generated text outputs from the LM. This labelling process would produce data that looks like (prompt, winning_response, losing_repsonse). Given such data, the objective of the Reward Model LM is to maximize the difference in score between winning response and the losing response.

$$r(x,y) = Reward$$
Given (x, $y_w$, $y_l$) , where x is the prompt, $y_w$ is the winning response and $y_l$ is the lossing response, we optimise the loss
$$ Loss = -log[\sigma(r(x,y_w)-r(x,y_l))]$$
We have two cases :
- $r(x,y_w) > r(x,y_l))$ -> Sigmoid >0.5 -> Loss will be small
- $r(x,y_w) < r(x,y_l))$ -> Sigmoid <0.5 -> Loss will be very high
This loss forces model to give high rewards to the winning response and low rewards the losign responses


### Fine-tuning with RL

Some parameters of the LM are frozen because fine-tuning an entire 10B or 100B+ parameter model is prohibitively expensive (for more, see Low-Rank Adaptation (LoRA) for LMs or the Sparrow LM from DeepMind) -- depending on the scale of the model and infrastructure being used. 

We need to calculate gradient for gradient ascent
$$\hat{g} = 1/|D| \sum_{\tau \in D} \sum_{t=0}^T \nabla_\theta log \pi_\theta(a_t|S_t) R(\tau) $$


Generate multiple responses for a prompt using high Temperature or random sampling using max_p. 

Given a prompt, response $\{o_i\}$ pair 
$$log \pi_\theta(o_{t+1}|Prompt+\{o_0..o_t\}) = log(softmax(o_{t+1}))$$

calculate reward score using the reward model at each output token with a discount factor $\gamma$
$$R(\tau) = \sum_{t=0}^T\gamma^tr_t$$


#### Finetuning LLM with RL
![LM Policy](/assets/images/rlhf_images/LM_Policy.png)

### Problems with Gradient Policy
Gradient descent will tell us that we need to take a step to the right, in this case. If we take a regular step to the right, it’s alright. We’re still taking a step toward the optimal policy. However, if we take a slightly larger step, we end up in an entirely different pit. We generally don’t want that.

![Gradient Ascent](/assets/images/rlhf_images/TRPO.png)

In supervised learning, this is usually not too big of a problem because we have the correct label for all our data. So, even if we step out of our initial pit because of one label, we are usually brought back in by the remaining labels. However, this is usually not the case with reinforcement learning. If we take a wrong step in gradient descent, our outputs may lead to an incorrect action. This incorrect action may then take us to a bad state. All experiences from here on out, until the end of the episode, may well be pointless. It’s more difficult to recover from this because the data we get for training depends on the current policy. Since the policy is constantly changing, we can say that the data is **non-stationary**. In other words, the data being trained after each epoch is different.

One more issue with the Vanilla policy gradient is that , we are optimising to increase the probability of paths with positive rewards while decreasing the probability of paths with negative rewards. What if Reward scores are all positive. Ideally we should be increasing the probability of paths which would result in above average rewards and decrease for below average rewards.
