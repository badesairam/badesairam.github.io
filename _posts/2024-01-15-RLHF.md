# Learning from Human Feedback
## Table of Contents

1. [Language Model](#language-model)
2. [Reinforcement Learning](#reinforcement-learning)
3. [Language Model as an Agent](#language-model-as-an-agent)
    * [Reward Model](#reward-model)
    * [Fine-tuning with RL](#fine-tuning-with-rl)
4. [Problems with Gradient Policy](#problems-with-gradient-policy)
    * [High Variance Gradients](#high-variance)
    * [Non-stationary Data](#non-stationary-data)
5. [PPO](#ppo)
6. [Conclusion](#conclusion)

Making language models bigger does not inherently make them better at following a user's intent. Wouldn't it be great if we could use human feedback for generated text as a measure of performance or go even one step further and use that feedback to optimize the model for the user's intent? That's the idea of Reinforcement Learning from Human Feedback (**RLHF**). RL has been very difficult to work with and, therefore, has been confined to gaming and simulated environments.

In this post, let's go over how exactly RLHF works, why it works, and the intuition behind RLHF. Let's also use the trlX library to implement RLHF for the summarization task.

## Language Model
A language model encodes statistical information about language. Statistical information tells us how likely a token (usually a sub-word) appears in a given context. It can be defined as

$P(nexttoken \mid prompt)$ 

Pretraining optimizes for completion. Making LMs bigger does not inherently make them better at following a user's intent. For example, if you give a pretrained model a question, ```How to make pizza```, any of the following could be a valid completion:
1. Adding more context to the question: ```for a family of six```
2. Adding follow-up questions: ```?How much time would it take?```
3. Actually giving the answer.

We can use **supervised fine-tuning (SFT)** to optimize/align the pretrained model to generate the responses that users are looking for. We can use human-curated demonstration data for this, often referred to as behavior cloning.

Demonstration data is hard to collect, and as dialogues are often flexible, they result in plausible responses rather than a single answer for a prompt. So we would like the model to learn human preferences. In order to achieve this, we will employ Reinforcement Learning.

SFT and RLHF from the InstructGPT paper
![InstructGPT](/assets/images/rlhf_images/instructGPT.png)

## Reinforcement Learning

The aim of Reinforcement Learning is to determine how an intelligent agent should take actions in an environment to maximize the overall reward. In game systems, the reward achieved is very clear.

![RL system](/assets/images/rlhf_images/RL.png)

A policy rules how the agent selects the action to perform given the state $a_{t}$ ~ $\pi(.|s_{t})$. The aim is to identify an optimal policy that maximizes the expected reward J given policy $\pi_\theta$, parameterized by parameters $\theta$:

$$ \pi^* = argmax_\pi J(\pi_\theta) $$  
$$J(\pi) = E[R(\tau)] = \int_\tau P(\tau|\pi)R(\tau)$$

We can use stochastic gradient ascent to achieve this maximum.

**Policy Gradient Optimization**

$$ \theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\pi_\theta)| \theta_k$$

The probability of taking a trajectory within a policy $\pi$ can be defined as:
$$ P(\tau|\pi_\theta) = P_0(S_0) \prod_{t=0}^T P(S_{t+1}|a_t, S_t) \pi_\theta(a_t|S_t) $$

We can simplify the policy gradient $\nabla_\theta J(\pi_\theta)$ using (attaching the derivation below):
$$\nabla_\theta J(\pi_\theta) = E[\sum_{t=0}^T \nabla_\theta log \pi_\theta(a_t|S_t) R(\tau)] $$

This expectation can be estimated as a sample mean by collecting some $D= \{\tau_i\}$ trajectories:
$$\hat{g} = 1/|D| \sum_{\tau \in D} \sum_{t=0}^T \nabla_\theta log \pi_\theta(a_t|S_t) R(\tau) $$

**REINFORCE ALGORITHM for Policy Optimization**

1. Create a neural network with parameters $\theta$ that defines a policy.
2. Use the neural network to sample trajectories and their rewards.
3. Calculate the gradient $\hat{g}$.
4. Run Stochastic Ascent to update parameters $\theta$.
5. Repeat Step 2.


**Policy Gradient Simplification**

![policy_gradient](/assets/images/rlhf_images/policy_gradient.png)

Now let's get back to our Language Model.

## Language Model as an Agent
We can consider a language model as an agent, with the current prompt representing the state and the next token selection considered as an action. The language model should be rewarded for generating "good responses (human preferred)."

The policy is the language model itself:

$a_{t}$ ~ $\pi(.|s_{t})$ = $P(nexttoken \mid prompt)$

States and actions in a Language model, $\tau = \{s_i\}$ 
![LM States](/assets/images/rlhf_images/LM_states.png)

But how do we determine the reward (a scalar) that needs to be given for a response to a prompt? We exploit the fact that humans are good at comparing but not necessarily agreeing to reach a common ground. The idea is: what if we have a scoring function that, when given a prompt and a response, outputs a score for how "good" that response is? This scoring function, called the reward model or preference model, calibrated with human preferences, can be used to train our LLMs towards giving responses with high scores. The output being a scalar reward is crucial for existing RL algorithms to be integrated seamlessly later in the RLHF process.

The RLHF process involves:
1) Training a reward model to generate a score given a prompt-response pair.
2) Optimizing the LLM to generate responses for which the reward model will give high scores using a policy-gradient RL algorithm, Proximal Policy Optimization (PPO).

### Reward Model
The reward model can be either another fine-tuned LM or an LM trained from scratch on the preference data. For example, Anthropic has used a specialized method of fine-tuning to initialize these models after pretraining (preference model pretraining, PMP) because they found it to be more sample efficient than fine-tuning. However, no single base model is considered the clear best choice for reward models. There will be one linear layer to produce one score.

![RM Model](/assets/images/rlhf_images/RewardModel.png)

Human annotators are used to rank the generated text outputs from the LM. This labeling process produces data that looks like (prompt, winning_response, losing_response). Given such data, the objective of the Reward Model LM is to maximize the difference in score between the winning response and the losing response.

$$r(x,y) = Reward$$
Given (x, $y_w$, $y_l$), where x is the prompt, $y_w$ is the winning response, and $y_l$ is the losing response, we optimize the loss:
$$ Loss = -log[\sigma(r(x,y_w)-r(x,y_l))]$$
We have two cases:
- $r(x,y_w) > r(x,y_l)$ -> Sigmoid >0.5 -> Loss will be small.
- $r(x,y_w) < r(x,y_l)$ -> Sigmoid <0.5 -> Loss will be very high.

This loss forces the model to give high rewards to the winning response and low rewards to the losing responses.

Additionally, Llama 2 proposed a margin loss as a regularizer for reward model training: $$Loss = -log[\sigma(r(x,y_w)-r(x,y_l))-m(r)]$$
Where $m(r)$ is the numerical difference in delta between the ratings of two annotators. This is either achieved by having annotators rate the outputs on a numerical scale or by using a quantified ranking method, such as Likert scales.

### Fine-tuning with RL

During this process, some parameters of the LM are frozen because fine-tuning an entire 10B or 100B+ parameter model is prohibitively expensive, depending on the scale of the model and infrastructure being used.

We need to calculate the gradient for gradient ascent:
$$\hat{g} = 1/|D| \sum_{\tau \in D} \sum_{t=0}^T \nabla_\theta log \pi_\theta(a_t|S_t) R(\tau) $$

Generate multiple responses for a prompt using high temperature or random sampling using max_p.

Given a prompt-response $\{o_i\}$ pair:
$$log \pi_\theta(o_{t+1}|Prompt+\{o_0..o_t\}) = log(softmax(o_{t+1}))$$

Calculate the reward score using the reward model at each output token with a discount factor $\gamma$:
$$
R(\tau) = \sum_{t=0}^T\gamma^tr_t
$$

#### Fine-tuning LLM with RL
![LM Policy](/assets/images/rlhf_images/LM_policy.png)

## Problems with Gradient Policy

#### High Variance
The gradient steps have very high variance, leading to noisy updates.

##### Reducing Variance
1. **Rewards to go**: 
    The estimator of the gradient multiplies the gradient of log probabilities of each action in the current trajectory with the rewards of the entire trajectory. As we know, actions cannot alter the rewards before that action, so we are also multiplying rewards that came before this action. Therefore, we can remove the rewards before this action and only consider rewards to go.

    $$\nabla_\theta J(\pi_\theta) = E[\sum_{t=0}^T \nabla_\theta log \pi_\theta(a_t|S_t) R(\tau)] $$
    can be approximated to:
    $$\hat{g} = 1/|D| \sum_{\tau \in D} \sum_{t=0}^T \nabla_\theta log \pi_\theta(a_t|S_t) \sum_{t`=t}^T r(s_{i,t`}, a_{i,t`}) $$

    We can derive the reward-to-go policy gradient using the [expected grad-log-prob (EGLP) lemma](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#expected-grad-log-prob-lemma).

    This reduces the variance as we remove unnecessary elements from the summation. 
2. **Baseline subtraction: Advantage**:
    One more issue with the vanilla policy gradient is that we are optimizing to increase the probability of paths with positive rewards while decreasing the probability of paths with negative rewards. What if reward scores are all positive? Ideally, we should be increasing the probability of paths that would result in above-average rewards and decreasing for below-average rewards. 

    
    So what should this baseline be?

    **Value function** $V^\pi(s)$: Provides the estimated reward we can get from state s.
    $$V^\pi(s) = E_{\pi}[R_t|s_t=s]$$

    We can add an additional layer on top of our LLM to estimate $V^\pi(s)$. 

    After subtracting the baseline, we get:

    $$\nabla_\theta J(\pi_\theta) = 1/|D| \sum_{\tau \in D} \sum_{t=0}^T \nabla_\theta log \pi_\theta(a_t|S_t) (\sum_{t`=t}^T r(s_{i,t`}, a_{i,t})-V_{\pi}(S_t))
     $$

    After the subtraction of the baseline, we get the advantage $A_{\pi}(s,a)$, which tells us how better to choose a particular action in state s over the average expectation. 

    $$\nabla_\theta J(\pi_\theta) = 1/|D| \sum_{i=1}^{|D|} (\sum_{t=0}^T \nabla_\theta log \pi_\theta(a_t|S_t)) A_{\pi}(S_{i,t},a_{i,t})
     $$

3. **Surrogate Loss**:

    Collect data from the old policy to step more efficiently in the new policy.

    $$ J(\pi_\theta) = E_{\tau \in \theta}[R(\tau)] = E_{\tau \in \theta_{offline}}[\dfrac{P(\tau|\pi_{\theta})}{P(\tau|\pi_{\theta_{offline}})} R(\tau)]$$

    If $\theta = \theta_{offline}$, then $J(\theta)$ is the same as the vanilla policy gradient.

    Importance sampling and surrogate loss training:

    1. Collect trajectories (states and actions) from the LLM determined by $\theta_{offline}$.
    2. Set $\theta = \theta_{offline}$.
    3. Take a mini-batch of trajectories from step 2.
    4. Calculate the rewards, log probabilities, and advantage using $\theta$.
    5. Calculate the surrogate loss:
        $$\nabla_\theta J(\pi_\theta) = \dfrac{1}{m} \sum_{i=1}^{m} (\sum_{t=0}^T \nabla_\theta \dfrac{log \pi_\theta(a_t|S_t))}{log \pi_{\theta_{offline}}(a_t|S_t))} A_{\pi}(S_{i,t},a_{i,t})
     $$
    6. Run Stochastic Ascent to update parameters $\theta$.
    7. Repeat steps 3-7 for a few epochs.
    8. Again, set $\theta = \theta_{offline}$ after a few epochs and continue further training.

#### Non-stationary Data
Gradient descent will tell us that we need to take a step to the right in this case. If we take a regular step to the right, it's alright. We're still taking a step toward the optimal policy. However, if we take a slightly larger step, we end up in an entirely different pit. We generally don't want that.

![Gradient Ascent](/assets/images/rlhf_images/TRPO.png)

In supervised learning, this is usually not too big of a problem because we have the correct label for all our data. So, even if we step out of our initial pit because of one label, we are usually brought back in by the remaining labels. However, this is usually not the case with reinforcement learning. If we take a wrong step in gradient descent, our outputs may lead to an incorrect action. This incorrect action may then take us to a bad state. All experiences from here on out, until the end of the episode, may well be pointless. It's more difficult to recover from this because the data we get for training depends on the current policy. Since the policy is constantly changing, we can say that the data is **non-stationary**. In other words, the data being trained after each epoch is different. 

This is where TRPO comes in. The first two words of the phrase "Trust Region Policy Optimization" give an intuition of what it does. Simply put, the main idea is to limit the difference between our current policy and the new policy. "Trust Region" refers to this space around the predictions of our current policy, which we "trust" to be an acceptable prediction. More on [TRPO](https://dilithjay.com/blog/trpo). TRPO prevents policy networks from updating too large but is impractical to implement.

### PPO

PPO is motivated by the same question as TRPO: how can we take the biggest possible improvement step on a policy using the data we currently have, without stepping so far that we accidentally cause performance collapse? While TRPO tries to solve this problem with a complex second-order method, PPO is a family of first-order methods that use a few other tricks to keep new policies close to old ones. PPO methods are significantly simpler to implement and empirically seem to perform at least as well as TRPO.

We don't want to forget the gains made by pretraining in LLMs. 

#### PPO Loss
We clip the gradient update so that we don't take a large step, bounding $\dfrac{\pi_\theta(a_t|S_t))}{\pi_{\theta_{offline}}(a_t|S_t))}$ by $1-\epsilon$ or  $1+\epsilon$.

PPO
![PPO Clip](/assets/images/rlhf_images/PPO_clip.png)

If we apply the PPO algorithm above to train the model, the language model may output whatever the reward model is optimized to return. This might be produced at the cost of generating legible responses. So we penalize the reward obtained by KL divergence between the logits of the unaligned LLM and the fine-tuned LLM model.

$$ r(x, y) = r(x,y) - \beta log\dfrac{p_{\theta}(y|x)}{p_{\theta_{old}}(y|x)} $$

Some RLHF systems have added additional terms to the reward function. For example, OpenAI successfully experimented on InstructGPT by mixing in additional pre-training gradients (from the human annotation set) into the update rule for PPO. It is likely that as RLHF is further investigated, the formulation of this reward function will continue to evolve. 

### Conclusion
Putting it all together

![RLHF : PPO](/assets/images/rlhf_images/final.png)

While RLHF is extremely promising and impactful, gathering human preference data is quite expensive due to the direct integration of other human workers outside the training loop. Even though PPO is an effective online RLHF training algorithm used to train state-of-the-art models such as GPT-4, it can be quite challenging to use in practice due to its high GPU memory requirements. In particular, PPO needs to load four copies of the models into memory: 1) the policy model, 2) the reference policy model, 3) the reward model, and 4) the value model, as shown in the following figure. PPO also has many subtle implementation details that can be difficult to get right. Balancing exploration (trying new actions to discover better ones) and exploitation (choosing the best-known actions) is a fundamental challenge in RL. PPO tends to become progressively less exploratory as training proceeds, which can lead to getting stuck in local optima.

We are still in the early stages of LLMs, including RLHF. It is likely that RLHF will be further investigated and will evolve.

Writing this blog has been an enjoyable experience, as it has allowed me to delve into the intricacies of mathematical concepts in a clear and concise manner. I would like to extend my gratitude to [Umar Jamail's YouTube video](https://www.youtube.com/watch?v=qGyFrqc34yc&t=6497s&ab_channel=UmarJamil), which has been a significant source of inspiration for the content presented in this blog. 

### References
1. [RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html)
2. [Umar Jamil's YouTube video on RLHF](https://www.youtube.com/watch?v=qGyFrqc34yc&ab_channel=UmarJamil)
3. [Stanford Online's YouTube video on RLHF](https://www.youtube.com/watch?v=9vM4p9NN0Ts&ab_channel=StanfordOnline)
4. [Hugging Face's Blog on RLHF](https://huggingface.co/blog/rlhf)
5. [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
6. [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325)
7. [Proximal policy optimization algorithms](https://arxiv.org/abs/1707.06347)
8. [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)

