# Learning from Human Feedback
## Table of Contents

1. [Language Model](#language-model)
2. [Reinforcement Learning](#reinforcement-learning)
3. [Language Model as an Agent](#language-model-as-an-agent)
    * [Reward Model](#reward-model)
    * [Fine-tuning with RL](#fine-tuning-with-rl)
4. [Problems with Gradient Policy](#problems-with-gradient-policy)
    * [High Variance Graidents](#high-variance)
    * [Non-stationary data](#non-stationary-data)
5. [PPO](#ppo)
6. [Conclusion](#conclusion)

Making language models bigger does not inherently make them better at following a user's intent. Wouldn't it be great if we use human feedback for generated text as a measure of performance or go even one step further and use that feedback to optimise the model for user's intent? That's the idead of Reinforcement Learning from Human Feedback (**RLHF**). RL has been very difficult to wotk with, and therefore, confined to gaming and simulated environments. 

In this post lets go over how exactly does RLHF work? why does it work? and intuition behind RLHF. Lets also use trlX library to implement RLHF for summarization task


## Language Model
A language model encodes statistical information about language. Statistical information tells us how likely a token(usually a sub-word) appear in a given context. It can be defined as

$P(nexttoken \mid prompt)$ 

Pretraining optimizes for completion. making LMs bigger does not inherently make them better at following a user's intent. For example, if you give pretrained model a question, ```How to make pizza```, any of the following could be a valid completion
1. Adding more context to the question: ```for a family of six```
2. Adding follow-up questions: ```?How much time would it take?```
3. Actually giving the answer

We can use **supervised finetuning(SFT)** to optimize/align the pretrained model to generate the responses that users are looking for. We can use human-curated demonstration data for this, often referred as behavior cloning.

Demonstration data is hard to collect and as dialogues are often felxible resulting in plausible responses rather than a single answer for a prompt. So we would like the model to learn human-preference. Inorder to achieve this , we will employ Reinforce Learning

                                             
SFT and RLHF from InstructGPT paper
![InstructGPT](/assets/images/rlhf_images/instructGPT.png)


## Reinforcement Learning

Aim of Reinforcement Learning is how an intelligent agent should take actions in an environment to maximize the overall reward. In game systems reward achieved is very clear.

![RL system](/assets/images/rlhf_images/RL.png)


A policy rules how the agent selects the action to perform given the state $a_{t}$ ~ $\pi(.|s_{t})$ . 
Aim is to identify an optimal policy which maximizes the expected reward J given policy $\pi_\theta$, parametrized by parameters $\theta$
$$ \pi^* = argmax_\pi J(\pi_\theta) $$  
$$J(\pi) = E[R(\tau)] = \int_\tau P(\tau|\pi)R(\tau)$$

We can use stochastic gradient ascent to achieve this maxima 

**Policy Gradient optimisation**

$$ \theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\pi_\theta)| \theta_k$$

Probability of taking a trajectory within a policy $\pi$ can be defined as
$$ P(\tau|\pi_\theta) = P_0(S_0) \prod_{t=0}^T P(S_{t+1}|a_t, S_t) \pi_\theta(a_t|S_t) $$

We can simplicify policy gradient $\nabla_\theta J(\pi_\theta)$ using (attaching the derivation below)
$$\nabla_\theta J(\pi_\theta) = E[\sum_{t=0}^T \nabla_\theta log \pi_\theta(a_t|S_t) R(\tau)] $$

This expectation can be estimated as sample mean by collecting some $D= \{\tau_i\}$ trajectories
$$\hat{g} = 1/|D| \sum_{\tau \in D} \sum_{t=0}^T \nabla_\theta log \pi_\theta(a_t|S_t) R(\tau) $$

REINFORCE ALGORITHM for policy optimisation
```
1. Create a neural network with parameters $\theta$ that defines a policy
2. Use the neural network to sample trajectories and their rewards
3. Calculate the gradient $\hat{g}$ 
4. Run Stochastic Ascent to update parameters $\theta$
5. Repeat Step 2
```

Policy Gradient Simplification

![policy_gradient](/assets/images/rlhf_images/policy_gradient.png)

Now lets get back to our Langauge Model

## Language Model as an Agent
We can consider a language model as an agent with the current prompt representing state and the next token selection can be considered as an Action. The language model should be rewareded for generating "good responses (human preferred)".  

Policy is the langauge model itself

$a_{t}$ ~ $\pi(.|s_{t})$ = $P(nexttoken \mid prompt)$

States and actions in a Language model , $\tau = \{s_i\}$ 
![LM States](/assets/images/rlhf_images/LM_states.png)

But how to determine the reward(a scalar) that needs to given for a reponse for prompt ? We exploit the fact that us humans are good at comparing but not agreeing to reach a common ground. The idea is what if we have a scoring function that , if given a prompt and a reponse -> outputs a score for how "good" that reposnse. This scoring function called reward model or preference model calibrated with human preferences can be used to train our LLMs towards giving reponses with high scores. The output being a scalar reward is crucial for existing RL algorithms being integrated seamlessly later in the RLHF process.

RLHF process involves
1) Training a Reward model to generate a score given prompt, response pair
2) Optimize the LLM to generate responses for which the reward model will give high scores with a policy-gradient RL algorithm, Proximal Policy Optimization (PPO).

### Reward Model
Reward model can be both another fine-tuned LM or a LM trained from scratch on the preference data. For example, Anthropic has used a specialized method of fine-tuning to initialize these models after pretraining (preference model pretraining, PMP) because they found it to be more sample efficient than fine-tuning, but no one base model is considered the clear best choice for reward models. There will be one linear layer to produce 1 score

![RM Model](/assets/images/rlhf_images/RewardModel.png)

Human annotators are used to rank the generated text outputs from the LM. This labelling process would produce data that looks like (prompt, winning_response, losing_repsonse). Given such data, the objective of the Reward Model LM is to maximize the difference in score between winning response and the losing response.

$$r(x,y) = Reward$$
Given (x, $y_w$, $y_l$) , where x is the prompt, $y_w$ is the winning response and $y_l$ is the lossing response, we optimise the loss
$$ Loss = -log[\sigma(r(x,y_w)-r(x,y_l))]$$
We have two cases :
- $r(x,y_w) > r(x,y_l))$ -> Sigmoid >0.5 -> Loss will be small
- $r(x,y_w) < r(x,y_l))$ -> Sigmoid <0.5 -> Loss will be very high
This loss forces model to give high rewards to the winning response and low rewards the losign responses

Additionally, Llama 2 proposed a margin loss as a regularizer for reward model training: $$Loss = -log[\sigma(r(x,y_w)-r(x,y_l))-m(r)]$$
Where $m(r)$ is the numerical difference in delta between the ratings of two annotators. This is either achieved by having annotators rate the outputs on a numerical scale or by using a quantified ranking method, such as Likert scales.


### Fine-tuning with RL

Some parameters of the LM are frozen because fine-tuning an entire 10B or 100B+ parameter model is prohibitively expensive -- depending on the scale of the model and infrastructure being used. 

We need to calculate gradient for gradient ascent
$$\hat{g} = 1/|D| \sum_{\tau \in D} \sum_{t=0}^T \nabla_\theta log \pi_\theta(a_t|S_t) R(\tau) $$


Generate multiple responses for a prompt using high Temperature or random sampling using max_p. 

Given a prompt, response $\{o_i\}$ pair 
$$log \pi_\theta(o_{t+1}|Prompt+\{o_0..o_t\}) = log(softmax(o_{t+1}))$$

calculate reward score using the reward model at each output token with a discount factor $\gamma$
$$
R(\tau) = \sum_{t=0}^T\gamma^tr_t
$$


#### Finetuning LLM with RL
![LM Policy](/assets/images/rlhf_images/LM_policy.png)

## Problems with Gradient Policy

#### High Variance
The gradient steps have very high variance leading to noisy updates

##### Reducing Variance
1. **Rewards to go** : 
    Estimator of the gradient is multiplying the gradient of log probabilities of each action in the current trajectory with the rewards of the entire trajectory. As we know action cannot alter the rewards before that action, we are also multiplying rewards that came before the action was taken. So we can remove the rewards before this action and only consider rewards to go.

    $$\nabla_\theta J(\pi_\theta) = E[\sum_{t=0}^T \nabla_\theta log \pi_\theta(a_t|S_t) R(\tau)] $$
    can be approximated to
    $$\hat{g} = 1/|D| \sum_{\tau \in D} \sum_{t=0}^T \nabla_\theta log \pi_\theta(a_t|S_t) \sum_{t`=t}^T r(s_{i,t`}, a_{i,t`}) $$

    We can derive the reward-to-go policy gradient using the [expected grad-log-prob (EGLP) lemma](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#expected-grad-log-prob-lemma).

    This reduces the variance as we remove unncessary elements from the summation. 
2. **Baseline subtraction : Advantage** :
    One more issue with the Vanilla policy gradient is that , we are optimising to increase the probability of paths with positive rewards while decreasing the probability of paths with negative rewards. What if Reward scores are all positive. Ideally we should be increasing the probability of paths which would result in above average rewards and decrease for below average rewards. 

    
    So what does should be this baseline??

    **Value function** $V^\pi(s)$ : Provides the estimated reward we can get from state s.
    $$V^\pi(s) = E_{\pi}[R_t|s_t=s]$$

    we can additional layer on top of our LLM to estimate $V^\pi(s)$. 

    After subtracting the baseline , we get

    $$\nabla_\theta J(\pi_\theta) = 1/|D| \sum_{\tau \in D} \sum_{t=0}^T \nabla_\theta log \pi_\theta(a_t|S_t) (\sum_{t`=t}^T r(s_{i,t`}, a_{i,t`})-V_{\pi}(S_t))
     $$

    After the subtracrtion of baseline we get advantage $A_{\pi}(s,a)$, tells us how better to choose a particular action iu state s over the average expectation. 

    $$\nabla_\theta J(\pi_\theta) = 1/|D| \sum_{i=1}^{|D|} (\sum_{t=0}^T \nabla_\theta log \pi_\theta(a_t|S_t)) A_{\pi}(S_{i,t},a_{i,t})
     $$

3. Surrogate Loss

    Collect data from old policy to step more effeciently in new policy

    $$ J(\pi_\theta) = E_{\tau \in \theta}[R(\tau)] = E_{\tau \in \theta_{offline}}[\dfrac{P(\tau|\pi_{\theta})}{P(\tau|\pi_{\theta_{offline}})} R(\tau)]$$

    if $\theta = \theta_{offline}$ then $J(\theta)$ is same as vanilla policy gradient

    Importance sampling and surrogate loss training

    1. Collect trajectories (states and actions) from LLM determined by $\theta_{offline}$
    2. set $\theta = \theta_{offline}$
    3. Take a mini_batch of trajectories from 2
    4. Calculate the rewards , log probablities and advantage using  $\theta$
    5. Calculate the surrogate loss
        $$\nabla_\theta J(\pi_\theta) = \dfrac{1}{m} \sum_{i=1}^{m} (\sum_{t=0}^T \nabla_\theta \dfrac{log \pi_\theta(a_t|S_t))}{log \pi_{\theta_{offline}}(a_t|S_t))} A_{\pi}(S_{i,t},a_{i,t})
     $$
    6. Run Stochastic Ascent to update parameters $\theta$
    7. Repeat Step 3-7 for few epochs
    8. again set $\theta = \theta_{offline}$ after few epochs and continue further training


#### Non-stationory data
Gradient descent will tell us that we need to take a step to the right, in this case. If we take a regular step to the right, it's alright. We're still taking a step toward the optimal policy. However, if we take a slightly larger step, we end up in an entirely different pit. We generally don't want that.

![Gradient Ascent](/assets/images/rlhf_images/TRPO.png)

In supervised learning, this is usually not too big of a problem because we have the correct label for all our data. So, even if we step out of our initial pit because of one label, we are usually brought back in by the remaining labels. However, this is usually not the case with reinforcement learning. If we take a wrong step in gradient descent, our outputs may lead to an incorrect action. This incorrect action may then take us to a bad state. All experiences from here on out, until the end of the episode, may well be pointless. It's more difficult to recover from this because the data we get for training depends on the current policy. Since the policy is constantly changing, we can say that the data is **non-stationary**. In other words, the data being trained after each epoch is different. 

This is where TRPO comes in. The first two words of the phrase "Trust Region Policy Optimization" gives an intuition of what it does. Simply put, the main idea is to limit the difference between our current policy and the new policy. "Trust Region" refers to this space around the predictions of our current policy which we "trust" to be an acceptable prediction. More on [TRPO](https://dilithjay.com/blog/trpo). TRPO prevents policy networks updates to be large but impractical to implement.

### PPO

PPO is motivated by the same question as TRPO: how can we take the biggest possible improvement step on a policy using the data we currently have, without stepping so far that we accidentally cause performance collapse? Where TRPO tries to solve this problem with a complex second-order method, PPO is a family of first-order methods that use a few other tricks to keep new policies close to old. PPO methods are significantly simpler to implement, and empirically seem to perform at least as well as TRPO.

We dont want to forget the gains made by pretraining in LLMs. 

#### PPO Loss
We clip the gradient update so that we dont take a large step bounding $\dfrac{\pi_\theta(a_t|S_t))}{\pi_{\theta_{offline}}(a_t|S_t))}$ by $1-\epsilon$ or  $1+\epsilon$

PPO
![PPO Clip](/assets/images/rlhf_images/PPO_clip.png)


If we apply the PPO algorithm above to train the model, language model may output whatever the reward model is optimised to return. This might be produced at the cost of producing legible responses. So we penalize the reward obtained by KL divergence between logits of unaligned LLM and the finetuned LLM model.

$$ r(x, y) = r(x,y) - \beta log\dfrac{p_{\theta}(y|x)}{p_{\theta_{old}}(y|x)} $$

Some RLHF systems have added additional terms to the reward function. For example, OpenAI experimented successfully on InstructGPT by mixing in additional pre-training gradients (from the human annotation set) into the update rule for PPO. It is likely as RLHF is further investigated, the formulation of this reward function will continue to evolve. 

### Conclusion
Putting it all together

![RLHF : PPO](/assets/images/rlhf_images/final.png)

While RLHF are extremely promising and impactful, gathering the human preference data is quite expensive due to the direct integration of other human workers outside the training loop. Eventhough PPO is an effective online RLHF training algorithm that is used to train state-of-the-art models such as GPT-4. However, PPO can be quite challenging to use in practice due to its high GPU memory requirements. In particular, PPO needs to load 4 copies of the models into the memory: 1) the policy model, 2) the reference policy model, 3) the reward model, and 4) the value model, as shown in the following figure. PPO also has many subtle implementation details that can be difficult to get right. 

We are still in the early stages of LLMs, including RLHF. It is likely, RLHF is further investigated and will evolve.

Writing this blog has been an enjoyable experience, as it has allowed me to delve into the intricacies of mathematical concepts in a clear and concise manner. I would like to extend my gratitude to [Umar Jamail's YouTube video](https://www.youtube.com/watch?v=qGyFrqc34yc&t=6497s&ab_channel=UmarJamil), which has been a significant source of inspiration for the content presented in this blog. 


### References
1. [RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html)
2. [Umar Jamil's YouTube video on RLHF](https://www.youtube.com/watch?v=qGyFrqc34yc&ab_channel=UmarJamil)
3. [Stanford Online's YouTube video on RLHF](https://www.youtube.com/watch?v=9vM4p9NN0Ts&ab_channel=StanfordOnline)
4. [Hugging Face's Blog on RLHF](https://huggingface.co/blog/rlhf)

